{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f06a11",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0718688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ms(times_list):\n",
    "    '''Utility function for converting seconds to milliseconds'''\n",
    "    ms_time_list = []\n",
    "    for elt in times_list:\n",
    "        ms_time_list.append([int(elt[0]*1000),int(elt[1]*1000)])\n",
    "    return ms_time_list\n",
    "\n",
    "def process_atc_label_line(line):\n",
    "    '''Utility function for parsing transcription file'''\n",
    "    lines_list = str.split(line, \"\\n\")\n",
    "    times_list = []\n",
    "    for elt in lines_list:\n",
    "        if elt[0:4] == \" (TI\":\n",
    "            num_string = ['1','2','3','4','5','6','7','8','9','0',' ','.']\n",
    "            new_elt = ''.join(c for c in elt if c in num_string)\n",
    "            new_elt_tok = new_elt.split() \n",
    "            new_elt_tok = [float(num) for num in new_elt_tok]\n",
    "            times_list.append(new_elt_tok)\n",
    "    return convert_to_ms(times_list)\n",
    "\n",
    "class atc_audio_file():\n",
    "    def __init__(self, name,audio_path, labels_path,new_flag = 0):\n",
    "        self.name = name\n",
    "        self.vad_slices = None\n",
    "        self.frames = None\n",
    "        self.frames_labels = None\n",
    "        self.mfcc = None\n",
    "        self.n_clips = 100\n",
    "        self.flag = new_flag\n",
    "        self.labels_path = labels_path\n",
    "        self.audio_path = audio_path\n",
    "    \n",
    "    def get_slices(self):\n",
    "        labels_path = self.labels_path\n",
    "        with open(labels_path, 'r') as f:\n",
    "            label = f.read()\n",
    "        label = process_atc_label_line(label)\n",
    "        self.vad_slices = label\n",
    "        return self.vad_slices\n",
    "            \n",
    "    def get_split_frames(self):\n",
    "        ms_2_sample = self.sample_rate/1000\n",
    "        frames_array = np.zeros(self.mfcc.shape[2]*self.n_clips)\n",
    "        \n",
    "        for v in self.vad_slices:\n",
    "            start = math.floor(v[0]*ms_2_sample)\n",
    "            end = math.ceil(v[1]*ms_2_sample)\n",
    "            for i in range(start,end):\n",
    "                n = min(math.floor(i/220),len(frames_array)-1)\n",
    "                j = i%220\n",
    "                if j <= 110:\n",
    "                    frames_array[n-2] += 1\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=111 and j<=220:\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=221 and j<=330:\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                    frames_array[n+1] += 1\n",
    "                elif j>=331 and j<=440:\n",
    "                    frames_array[n+1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=441:\n",
    "                    frames_array[n+2] += 1\n",
    "                    frames_array[n+1] += 1\n",
    "                    frames_array[n] += 1\n",
    "        \n",
    "        self.clip_size = self.mfcc.shape[2]\n",
    "        frame_arr_list = []\n",
    "        for j in range(self.n_clips):\n",
    "            frame_arr_list.append(np.expand_dims(frames_array[j*self.clip_size:(j+1)*self.clip_size],axis=0))\n",
    "        self.frames = np.concatenate(frame_arr_list,axis=0)\n",
    "        self.frames = np.clip(self.frames,0,1)\n",
    "        return self.frames\n",
    "    \n",
    "    def get_split_labels(self):\n",
    "        self.frames_labels = np.zeros_like(self.frames)\n",
    "        self.frames_labels[np.where(self.frames>0)] = 1\n",
    "        return self.frames_labels\n",
    "    \n",
    "    def get_split_mfcc(self):\n",
    "        if self.flag == 1:\n",
    "            file_name = self.name\n",
    "        else:\n",
    "            file_name = self.audio_path\n",
    "        waveform, sample_rate = torchaudio.load(file_name)\n",
    "        effects = [['rate', '22050']]\n",
    "        self.waveform, self.sample_rate = torchaudio.sox_effects.apply_effects_tensor(waveform, sample_rate, effects)\n",
    "        pad_array = torch.zeros((1,10000*self.sample_rate))\n",
    "        pad_array[:,:self.waveform.shape[1]] = self.waveform\n",
    "        self.waveform = pad_array\n",
    "        self.waveform = self.waveform[:,:4000*self.sample_rate]\n",
    "        clip_size = math.floor(self.waveform.shape[1]/self.n_clips)\n",
    "        n_clips = self.n_clips\n",
    "        mfcc_list = []\n",
    "        n_fft = 2048\n",
    "        win_length = 551\n",
    "        hop_length = 220\n",
    "        n_mels = 40\n",
    "        n_mfcc = 40\n",
    "        mfcc_transform = T.MFCC(\n",
    "                sample_rate=self.sample_rate,\n",
    "                n_mfcc=n_mfcc,\n",
    "                melkwargs={\n",
    "                  'n_fft': n_fft,\n",
    "                  'n_mels': n_mels,\n",
    "                  'hop_length': hop_length,\n",
    "                  'mel_scale': 'htk',\n",
    "                }\n",
    "            )\n",
    "        for i in range(n_clips):\n",
    "            mfcc_list.append(mfcc_transform(self.waveform[:,i*clip_size:(i+1)*clip_size]))\n",
    "        self.mfcc = torch.cat(mfcc_list)\n",
    "        return self.mfcc\n",
    "\n",
    "def process_file_atc0(filename,audio_path, labels_path):\n",
    "    fname = filename\n",
    "    audio_file = atc_audio_file(fname, audio_path, labels_path)\n",
    "    audio_file.get_slices()\n",
    "    mfcc = audio_file.get_split_mfcc()\n",
    "    frames = audio_file.get_split_frames()\n",
    "    print(filename)\n",
    "    return mfcc, frames    \n",
    "    \n",
    "def process_atc0_files(k=100):\n",
    "    input_list = []\n",
    "    labels_list = []\n",
    "    #paths = ['/project/graziul/data/corpora/atc0_comp/atc0_bos/data/audio/', '/project/graziul/data/corpora/atc0_comp/atc0_dca/data/audio/', '/project/graziul/data/corpora/atc0_comp/atc0_dfw/data/audio/']\n",
    "    paths = ['/project/graziul/data/corpora/atc0_comp/atc0_bos/data/audio/']\n",
    "    for idx,path in enumerate(paths):\n",
    "        for fpath in glob(path + '*.sph'):\n",
    "            if(idx > k):\n",
    "                break\n",
    "            filename = fpath[-12:-4]\n",
    "            label_file = path[:-6] + 'transcripts/' + filename + '.txt'\n",
    "            x,y = process_file_atc0(filename, fpath, label_file)\n",
    "            input_list.append(x)\n",
    "            labels_list.append(y)\n",
    "            idx = idx+1\n",
    "        if(idx>k):\n",
    "            break\n",
    "    input_list = torch.cat(input_list)\n",
    "    input_list = torch.transpose(input_list,1,2)\n",
    "    labels_list = torch.from_numpy(np.concatenate(labels_list,axis = 0)).float()\n",
    "    return input_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce2acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the MFCC data and the \"ground truth\" labels from the transcriptions\n",
    "input_list, labels_list = process_atc0_files()\n",
    "# Save the input data and labels\n",
    "torch.save(input_list, 'atc0_data')\n",
    "torch.save(labels_list, 'atc0_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676abf51",
   "metadata": {},
   "source": [
    "### Ensemble SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab7ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data \n",
    "input_list = torch.load('atc0_data')\n",
    "labels_list = torch.load('atc0_labels')\n",
    "\n",
    "# Shuffle the data and labels in unison \n",
    "x, y = shuffle(input_list, labels_list, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084cdbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a grid search on the \"first\" 1% of the data to determine the hyperparameters\n",
    "x_model_select = x[:1]\n",
    "y_model_select = y[:1]\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[0.1, 1, 10], 'gamma':[0.001, 0.01, 0.1, 1]}\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters, cv=2)\n",
    "clf.fit(x_model_select.reshape([4010,40]), y_model_select.reshape(4010))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4df20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "means = clf.cv_results_[\"mean_test_score\"]\n",
    "stds = clf.cv_results_[\"std_test_score\"]\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_[\"params\"]):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206da3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training data is split into 5 segments and a seperate SVM is trained on each\n",
    "svm1 = svm.SVC(kernel='rbf', C=0.1, gamma=0.001)\n",
    "svm1.fit(x[10:20].reshape([40100,40]), y[10:20].reshape(40100))\n",
    "svm2 = svm.SVC(kernel='rbf', C=0.1, gamma=0.001)\n",
    "svm2.fit(x[20:30].reshape([40100,40]), y[20:30].reshape(40100))\n",
    "svm3 = svm.SVC(kernel='rbf', C=0.1, gamma=0.001)\n",
    "svm3.fit(x[30:40].reshape([40100,40]), y[30:40].reshape(40100))\n",
    "svm4 = svm.SVC(kernel='rbf', C=0.1, gamma=0.001)\n",
    "svm4.fit(x[40:50].reshape([40100,40]), y[40:50].reshape(40100))\n",
    "svm5 = svm.SVC(kernel='rbf', C=0.1, gamma=0.001)\n",
    "svm5.fit(x[50:60].reshape([40100,40]), y[50:60].reshape(40100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each SVM makes predictions for the validation data and these predictions are used as the feature set for the 6th SVM\n",
    "svm1_predictions = svm1.predict(input_list[60:70].reshape([40100,40]))\n",
    "svm2_predictions = svm2.predict(input_list[60:70].reshape([40100,40]))\n",
    "svm3_predictions = svm3.predict(input_list[60:70].reshape([40100,40]))\n",
    "svm4_predictions = svm4.predict(input_list[60:70].reshape([40100,40]))\n",
    "svm5_predictions = svm5.predict(input_list[60:70].reshape([40100,40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d566d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_predictions = np.vstack([svm1_predictions, svm2_predictions, svm3_predictions, svm4_predictions, svm5_predictions])\n",
    "stacked_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48276815",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('ensemble_svm_stacked', stacked_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef995b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm6 = svm.SVC(kernel='rbf', C=0.1, gamma=0.001)\n",
    "svm6.fit(stacked_predictions.T, labels_list[60:70].reshape(40100))\n",
    "predicted = svm6.predict(stacked_predictions.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2d7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support(labels_list[60:70].reshape(40100), predicted, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d135cc2",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f474a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "x_np = x.numpy()\n",
    "x_tensorflow = tf.convert_to_tensor(x_np)\n",
    "\n",
    "y_np = y.numpy()\n",
    "y_tensorflow = tf.convert_to_tensor(y_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c198093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=40, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(tf.reshape(x_tensorflow[:80], [320800,40], name=None), tf.reshape(y_tensorflow[:80],[320800]), epochs=100, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('nn_100epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(tf.reshape(x_tensorflow[80:], [3689200,40], name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acd8436",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support(labels_list[80:].reshape(3689200), (predictions > .5).astype(int), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4077b382",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=40, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(tf.reshape(x_tensorflow[:800], [3208000,40], name=None), tf.reshape(y_tensorflow[:800],[3208000]), epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09467c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('nn_10epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105bb3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(tf.reshape(x_tensorflow[800:], [802000,40], name=None))\n",
    "precision_recall_fscore_support(labels_list[800:].reshape(802000), (predictions > .5).astype(int), average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a8b22",
   "metadata": {},
   "source": [
    "### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f50bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_data = TensorDataset(input_list[:800], labels_list[:800])\n",
    "test_data = TensorDataset(input_list[800:1000], labels_list[800:1000])\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "sequence_length = 4010 #28\n",
    "input_size = 40 #28\n",
    "hidden_size = 1024 #128\n",
    "num_layers = 4 #2\n",
    "num_classes = 2 #10\n",
    "num_epochs = 2\n",
    "learning_rate = 0.003\n",
    "\n",
    "# Bidirectional recurrent neural network\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # 2 for bidirection \n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = BiRNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (mfccs, labels) in enumerate(train_loader):\n",
    "        mfccs = mfccs.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(mfccs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
